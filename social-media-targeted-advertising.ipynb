{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis, Natural Language Processing, and Social Media Data in Python\n",
    "\n",
    "<a href=\"http://creativecommons.org/licenses/by-nc/4.0/\" rel=\"license\"><img style=\"border-width: 0;\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" alt=\"Creative Commons License\" /></a>\n",
    "This tutorial is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc/4.0/\" rel=\"license\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data As Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# load data as pandas dataframe from file\n",
    "irads = pd.read_csv(\"index_irads.csv\")\n",
    "\n",
    "irads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# load data as pandas dataframe from URL\n",
    "irads = pd.read_csv(\"https://raw.githubusercontent.com/kwaldenphd/social-media-targeted-advertising/main/index_irads.csv\")\n",
    "\n",
    "irads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data types\n",
    "irads.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dataframe info\n",
    "irads.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show summary statistics\n",
    "irads.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort ad vaues by cost (descending sort)\n",
    "\n",
    "irads.sort_values(by=[\"cost\"], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by number of impressions, descending\n",
    "\n",
    "irads.sort_values(by=\"impressions\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by number of clicks, descending\n",
    "\n",
    "irads.sort_values(by=\"clicks\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by number of impressions and cost (descending)\n",
    "\n",
    "irads.sort_values(by=['clicks', 'cost'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group titles based on exclusion category \n",
    "\n",
    "irads[['title', 'exclude']].groupby('exclude').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of ads by exclusion category\n",
    "\n",
    "irads['exclude'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of ads by age range\n",
    "\n",
    "irads['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more options on interacting with data in a `DataFrame`: https://github.com/kwaldenphd/pandas-machine-learning-intro#interacting-with-a-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other datasets you could explore:\n",
    "- [`interests.csv`](https://raw.githubusercontent.com/umd-mith/irads/master/analysis/interests.csv) (money, clicks, impressions, number of ads, interest)\n",
    "- [`people_who_match.csv`](https://raw.githubusercontent.com/umd-mith/irads/master/analysis/people_who_match.csv) (money, clicks, impressions, number of ads, people type who match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing This Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "\n",
    "interests = pd.read_csv(\"https://raw.githubusercontent.com/umd-mith/irads/master/analysis/interests.csv\")\n",
    "\n",
    "interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick visual check of the data using pandas built-in plotting function\n",
    "\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate plot\n",
    "interests.plot()\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# customize plot showing number of ads by cost\n",
    "\n",
    "interests.plot.scatter(x='money (RUB)', y='ads', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new data frame for ads with over 100000 clicks\n",
    "\n",
    "top_interests = interests.loc[interests['clicks']>100000]\n",
    "\n",
    "top_interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot number of impressions by interest\n",
    "top_interests.plot.barh(x='interest', y='impressions', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on plotting data in a `dataframe`: https://github.com/kwaldenphd/more-with-matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Data Frame to Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for not null values\n",
    "\n",
    "irads[irads[\"title\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame from not null fields\n",
    "text = irads[irads[\"description\"].notna()]\n",
    "\n",
    "text = text[\"description\"]\n",
    "\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write ad text column to txt file\n",
    "text.to_csv('irads_text.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis Using Voyant Tools\n",
    "\n",
    "<a href=\"http://voyant-tools.org/\">Voyant Tools</a> is an open-source web application developed by Stéfan Sinclair and Geoffrey Rockwell in 2003, with later contributions added by Andrew MacDonald, Cyril Briquet, Lisa Goddard, and Mark Turcato. While Voyant is one of the leading robust web-based textual analysis interfaces, it grew out of existing text analysis tools like HyperPo, Tapoware, and TACT. Voyant also offers <a href=\"https://github.com/sgsinclair/Voyant\">open-source code</a> that can be used to deploy the program on a server. Voyant users can upload text files from their computer, link to online text sources, or scrape the text off a webpage for analysis and visualization. Unlike more advanced, programming-oriented textual analysis programs like R and R Studio, Voyant gives users access to a range statistical analysis and visualization features without requiring significant technical knowledge.\n",
    "\n",
    "Download the newly-created text file.\n",
    "\n",
    "Open a web browser (preferably Firefox or Chrome) and navigate to the <a href=\"http://voyant-tools.org/\">Voyant Tools homepage</a>.\n",
    "\n",
    "<p align=\"center\"><a href=\"https://github.com/kwaldenphd/Voyant-tutorial/blob/master/screenshots/Capture_1.PNG?raw=true\"><img class=\"aligncenter size-large wp-image-549\" src=\"https://github.com/kwaldenphd/Voyant-tutorial/blob/master/screenshots/Capture_1.PNG?raw=true\" alt=\"\" width=\"676\" height=\"523\" /></a></p>\n",
    "\n",
    "Upload the file and click Reveal.\n",
    "\n",
    "<p align=\"center\"><a href=\"https://github.com/kwaldenphd/Voyant-tutorial/blob/master/screenshots/Capture_2.PNG?raw=true\"><img class=\"aligncenter size-large wp-image-550\" src=\"https://github.com/kwaldenphd/Voyant-tutorial/blob/master/screenshots/Capture_2.PNG?raw=true\" alt=\"\" width=\"676\" height=\"355\" /></a></p>\n",
    "\n",
    "Once a text or corpus has been uploaded, Voyant moves into its ‘default skin,’ or primary editing environment.\n",
    "\n",
    "For more on Voyant's interface and functionality: https://github.com/kwaldenphd/Voyant-tutorial/tree/SPN-285#editing-in-voyant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The Natural Language Toolkit (NLTK) is a collection of reusable Python tools (also known as a Python library that help researchers apply a set of computational methods to texts. The tools range from methods of breaking up text into smaller pieces, to identifying whether a word belongs in a given language, to sample texts that researchers can use for training and development purposes (such as the complete text of Moby Dick).\" [Zoë Wilkinson Saldaña, \"Sentiment Analysis for Exploratory Data Analysis,\" The Programming Historian 7 (2018), https://doi.org/10.46430/phen0079.]\n",
    "\n",
    "For more info on NLTK: https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk\n",
    "import sys \n",
    "!{sys.executable} -m pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import additional nltk components\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Options for Tokenizing Using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural Language Processing (NLP), \"tokenizing\" refers to the process of breaking a large text into smaller units (words or sentences) known as tokens.\n",
    "\n",
    "NLTK includes a wide range of options for tokenizing text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_str = str(text)\n",
    "\n",
    "# text_str\n",
    "    \n",
    "word_tokenize(text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text_str = str(text)\n",
    "\n",
    "# text_str\n",
    "    \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "tokenizer.tokenize(text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using WordPunctTokenizer \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "text_str = str(text)\n",
    "\n",
    "# text_str\n",
    "    \n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "tokenizer.tokenize(text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenize using RegexpTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "text_str = str(text)\n",
    "\n",
    "# text_str\n",
    "    \n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "regex_words = tokenizer.tokenize(text_str)\n",
    "\n",
    "regex_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the most effective tokenizing method for this data in combination with a few other data wrangling steps to output a unique list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text_str)\n",
    "\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# remove punctuation/special characters\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "# remove non-text content\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words = [w for w in words if not w in stop_words]\n",
    "\n",
    "# removes words with fewer than 3 characters\n",
    "# words = [word for word in words if len(word) > 3]\n",
    "\n",
    "# output cleaned list of words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then take that list of words and plot term frequency and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk components\n",
    "import nltk\n",
    "from nltk.corpus import webtext\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('webtext')\n",
    "\n",
    "# analyze term frequency/distribution\n",
    "data_analysis = nltk.FreqDist(words)\n",
    "\n",
    "data_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot term frequency/distribution for all terms\n",
    "data_analysis.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show term frequency/distribution for top 10 terms\n",
    "for word, frequency in data_analysis.most_common(10):\n",
    "    print(u'{};{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Discussion questions:\n",
    "- What kinds of things were you interested in exploring via this dataset?\n",
    "- How did you approach those questions using computational methods?\n",
    "  * This could focus on what you did in Python using `pandas` and/or `nltk`\n",
    "  * You could also think about insights gained from a graphical user interface programs like Voyant Tools\n",
    "- What kinds of insights were you able to determine?\n",
    "- How did interacting with this data using computational methods shape your understanding of the data?\n",
    "- Where would you go next?\n",
    "- How are you thinking about race and surveillance after engaging with this data?\n",
    "- Other questions/thoughts/observations\n",
    "\n",
    "# Lab Notebook Questions\n",
    "\n",
    "The lab notebook consists of a narrative that documents and describes your experience working through this lab.\n",
    "\n",
    "You can respond to/engage with other discussion questions included in the lab procedure.\n",
    "\n",
    "But specific questions for the lab notebook (from the \"Putting It All Together\" section):\n",
    "- What kinds of things were you interested in exploring via this dataset?\n",
    "- How did you approach those questions using computational methods?\n",
    "  * This could focus on what you did in Python using `pandas` and/or `nltk`\n",
    "  * You could also think about insights gained from a graphical user interface programs like Voyant Tools\n",
    "- What kinds of insights were you able to determine?\n",
    "- How did interacting with this data using computational methods shape your understanding of the data?\n",
    "- Where would you go next?\n",
    "- How are you thinking about race and surveillance after engaging with this data?\n",
    "- Other questions/thoughts/observations\n",
    "\n",
    "I encourage folks to include code + screenshots as part of that narrative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
